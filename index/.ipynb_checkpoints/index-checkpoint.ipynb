{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT)\n",
    "if not os.path.exists(\"indexdir\"):\n",
    "    os.mkdir(\"indexdir\")\n",
    "ix = create_in(\"indexdir\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "wd = 'downloaded_transcripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['00:00:03.169 --> 00:00:03.179', 'good afternoon everyone so my name is'],\n",
       " ['00:00:07.519 --> 00:00:07.529', 'Steven prowl I think I want to start'],\n",
       " ['00:00:11.749 --> 00:00:11.759', 'first with mentioned that we are more on'],\n",
       " ['00:00:13.820 --> 00:00:13.830', 'a flying science rather than wheelie'],\n",
       " ['00:00:18.830 --> 00:00:18.840', 'research and you know like we start with'],\n",
       " ['00:00:21.820 --> 00:00:21.830', 'one intern which is the first guy change'],\n",
       " ['00:00:24.200 --> 00:00:24.210', \"jiangsu but he couldn't come here today\"],\n",
       " ['00:00:28.730 --> 00:00:28.740', 'and I also want to take opportunity to'],\n",
       " ['00:00:31.099 --> 00:00:31.109', 'thanks Christina for the time she'],\n",
       " ['00:00:34.729 --> 00:00:34.739', 'working with us as a scientist advisor'],\n",
       " ['00:00:38.869 --> 00:00:38.879', 'last but not least of the person in you'],\n",
       " ['00:00:44.410 --> 00:00:44.420', 'know in the list is Jen phone thank you'],\n",
       " ['00:00:47.830 --> 00:00:47.840', 'so this is a flow of our you know like'],\n",
       " ['00:00:51.439 --> 00:00:51.449', 'our life today so we start with the'],\n",
       " ['00:00:53.590 --> 00:00:53.600', 'overview and then we go through the'],\n",
       " ['00:00:56.600 --> 00:00:56.610', 'problem statement challenges in our'],\n",
       " ['00:01:00.160 --> 00:01:00.170', 'approach we will spend more time on'],\n",
       " ['00:01:03.830 --> 00:01:03.840', 'architecture and also on the experiment'],\n",
       " ['00:01:08.000 --> 00:01:08.010', 'evaluation and a life analysis and and'],\n",
       " ['00:01:14.359 --> 00:01:14.369', 'then our conclusion so first row medical'],\n",
       " ['00:01:17.929 --> 00:01:17.939', 'error correction the DC purpose is to'],\n",
       " ['00:01:20.810 --> 00:01:20.820', 'correct the local errors in a world'],\n",
       " ['00:01:24.020 --> 00:01:24.030', 'order while at the local error we want'],\n",
       " ['00:01:26.560 --> 00:01:26.570', 'to correct in terms of spelling and'],\n",
       " ['00:01:30.789 --> 00:01:30.799', 'inflection the motivation that we'],\n",
       " ['00:01:35.060 --> 00:01:35.070', 'recognized so far is the reason neural'],\n",
       " ['00:01:40.340 --> 00:01:40.350', 'you know network nmt have substantially'],\n",
       " ['00:01:44.389 --> 00:01:44.399', 'improving over the phrase base machine'],\n",
       " ['00:01:47.300 --> 00:01:47.310', 'translation however I think is a'],\n",
       " ['00:01:50.980 --> 00:01:50.990', 'constraining of lack of the optimization'],\n",
       " ['00:01:56.240 --> 00:01:56.250', 'in the GC when enough lie so the'],\n",
       " ['00:01:59.950 --> 00:01:59.960', 'proposal architecture we come up with is'],\n",
       " ['00:02:03.649 --> 00:02:03.659', 'with the outcome that we all form the'],\n",
       " ['00:02:06.950 --> 00:02:06.960', 'previous neural network with the forty'],\n",
       " ['00:02:10.430 --> 00:02:10.440', 'five point one five score compared to'],\n",
       " ['00:02:11.200 --> 00:02:11.210', 'the'],\n",
       " ['00:02:15.270 --> 00:02:15.280', 'previous model based on the canal 14'],\n",
       " ['00:02:21.540 --> 00:02:21.550', 'data set so in terms of the problems pay'],\n",
       " ['00:02:25.050 --> 00:02:25.060', 'as you know that the GC is basically'],\n",
       " ['00:02:29.080 --> 00:02:29.090', 'cast the problem as machine machine'],\n",
       " ['00:02:32.800 --> 00:02:32.810', 'translation and I think the translation'],\n",
       " ['00:02:36.670 --> 00:02:36.680', 'is happen in undramatic all English'],\n",
       " ['00:02:39.370 --> 00:02:39.380', 'sentence to the correct ones based on'],\n",
       " ['00:02:44.590 --> 00:02:44.600', 'the GAO paper in 2010 and also have been'],\n",
       " ['00:02:47.140 --> 00:02:47.150', 'proving superior to the system based on'],\n",
       " ['00:02:51.090 --> 00:02:51.100', 'the local classifier with roses calf'],\n",
       " ['00:02:55.450 --> 00:02:55.460', 'Korea and God however the specific'],\n",
       " ['00:03:00.730 --> 00:03:00.740', 'challenge of the MT approach mainly one'],\n",
       " ['00:03:05.010 --> 00:03:05.020', 'is a stream latch vocabulary needed and'],\n",
       " ['00:03:08.170 --> 00:03:08.180', 'especially with large number of words in'],\n",
       " ['00:03:11.650 --> 00:03:11.660', 'languages and Unsoeld is corresponding'],\n",
       " ['00:03:15.780 --> 00:03:15.790', 'alteration the second part of this is to'],\n",
       " ['00:03:19.110 --> 00:03:19.120', 'deal with the capture of a different'],\n",
       " ['00:03:22.510 --> 00:03:22.520', 'level of information for different half'],\n",
       " ['00:03:25.480 --> 00:03:25.490', 'error particularly for example and'],\n",
       " ['00:03:28.660 --> 00:03:28.670', 'misspell and local remark correction you'],\n",
       " ['00:03:31.510 --> 00:03:31.520', 'need to capture the world level and sub'],\n",
       " ['00:03:35.980 --> 00:03:35.990', 'level information and also in the world'],\n",
       " ['00:03:39.070 --> 00:03:39.080', 'order or you know like the usage you'],\n",
       " ['00:03:42.150 --> 00:03:42.160', 'also have to do have global semantic'],\n",
       " ['00:03:47.110 --> 00:03:47.120', 'information that you need to capture so'],\n",
       " ['00:03:49.870 --> 00:03:49.880', 'the related quote in this area so far is'],\n",
       " ['00:03:54.820 --> 00:03:54.830', 'basically in the nmt basically like i'],\n",
       " ['00:03:57.820 --> 00:03:57.830', 'mentioned earlier is a substantial'],\n",
       " ['00:04:02.260 --> 00:04:02.270', 'improvement on top of the quality based'],\n",
       " ['00:04:06.670 --> 00:04:06.680', 'on the faces empty face based MT before'],\n",
       " ['00:04:10.420 --> 00:04:10.430', 'and the fundamental of the nmt is about'],\n",
       " ['00:04:13.480 --> 00:04:13.490', 'like basically sequin to sequin model as'],\n",
       " ['00:04:17.410 --> 00:04:17.420', 'you already know and restrictive or'],\n",
       " ['00:04:22.200 --> 00:04:22.210', 'calorie by the hive even world I feel'],\n",
       " ['00:04:24.610 --> 00:04:24.620', 'and in the'],\n",
       " ['00:04:30.760 --> 00:04:30.770', 'jeez it has you know Sakaguchi also'],\n",
       " ['00:04:32.770 --> 00:04:32.780', 'emphasized that the important of the'],\n",
       " ['00:04:35.290 --> 00:04:35.300', 'long distance are the knowledge of the'],\n",
       " ['00:04:38.379 --> 00:04:38.389', 'world dependencies is super important to'],\n",
       " ['00:04:44.080 --> 00:04:44.090', 'solving the problem on GEC however the'],\n",
       " ['00:04:47.260 --> 00:04:47.270', 'neural approach to the GCS is Calais as'],\n",
       " ['00:04:52.150 --> 00:04:52.160', 'less Explorer we noticed two prior work'],\n",
       " ['00:04:56.230 --> 00:04:56.240', 'so far one is a world level sequel to'],\n",
       " ['00:05:01.680 --> 00:05:01.690', \"sequin that's done by UN and visco\"],\n",
       " ['00:05:04.210 --> 00:05:04.220', 'however I think the approach is'],\n",
       " ['00:05:09.460 --> 00:05:09.470', 'half-life failed to to get like the'],\n",
       " ['00:05:12.760 --> 00:05:12.770', 'information ikaw for the or V case the'],\n",
       " ['00:05:18.240 --> 00:05:18.250', 'alpha vocabulary as well as the'],\n",
       " ['00:05:20.340 --> 00:05:20.350', 'generalization of the approach is'],\n",
       " ['00:05:25.240 --> 00:05:25.250', 'half-life limited for unseen world the'],\n",
       " ['00:05:28.080 --> 00:05:28.090', 'second approach is on a character base'],\n",
       " ['00:05:35.440 --> 00:05:35.450', 'sequin to sequin by Xie in 2016 and the'],\n",
       " ['00:05:37.560 --> 00:05:37.570', 'constraining of that is in terms of'],\n",
       " ['00:05:41.710 --> 00:05:41.720', 'effective life getting the world level'],\n",
       " ['00:05:46.839 --> 00:05:46.849', 'knowledge information so here we come'],\n",
       " ['00:05:50.800 --> 00:05:50.810', 'with our approach basically we propose a'],\n",
       " ['00:05:53.320 --> 00:05:53.330', 'novice have rich neural network with'],\n",
       " ['00:06:01.210 --> 00:06:01.220', 'attention method layer so the approach'],\n",
       " ['00:06:05.920 --> 00:06:05.930', 'is solving the hand the glass amount of'],\n",
       " ['00:06:09.939 --> 00:06:09.949', 'vocabulary especially in the ovary case'],\n",
       " ['00:06:14.710 --> 00:06:14.720', 'and what we do is we bring on of the you'],\n",
       " ['00:06:19.629 --> 00:06:19.639', 'know like contact contact your work into'],\n",
       " ['00:06:24.629 --> 00:06:24.639', 'the representation of unify vector'],\n",
       " ['00:06:28.240 --> 00:06:28.250', 'including his contacts and the second is'],\n",
       " ['00:06:31.330 --> 00:06:31.340', 'we also capture the different level of'],\n",
       " ['00:06:34.540 --> 00:06:34.550', 'information for different highs the'],\n",
       " ['00:06:37.300 --> 00:06:37.310', 'world level information is used to'],\n",
       " ['00:06:37.980 --> 00:06:37.990', 'correct'],\n",
       " ['00:06:41.730 --> 00:06:41.740', 'Global grandma and you know'],\n",
       " ['00:06:46.290 --> 00:06:46.300', 'fluency rec error and while the'],\n",
       " ['00:06:48.360 --> 00:06:48.370', 'character information is you for'],\n",
       " ['00:06:51.000 --> 00:06:51.010', 'correcting you know local error in term'],\n",
       " ['00:07:01.740 --> 00:07:01.750', 'of misspelled inflected error and the'],\n",
       " ['00:07:05.210 --> 00:07:05.220', 'approach work we do is to thing that we'],\n",
       " ['00:07:07.770 --> 00:07:07.780', 'propose in here one is the hybrid'],\n",
       " ['00:07:11.190 --> 00:07:11.200', 'architecture overcome the loss of'],\n",
       " ['00:07:15.720 --> 00:07:15.730', 'information in the world level and what'],\n",
       " ['00:07:18.210 --> 00:07:18.220', 'we do is compose on the presentation of'],\n",
       " ['00:07:21.870 --> 00:07:21.880', 'the overly words using the character and'],\n",
       " ['00:07:28.020 --> 00:07:28.030', 'in model the second novel idea what we'],\n",
       " ['00:07:30.840 --> 00:07:30.850', 'have is basically is a nested level of'],\n",
       " ['00:07:34.380 --> 00:07:34.390', 'tension architecture so that we can'],\n",
       " ['00:07:38.310 --> 00:07:38.320', 'address the you know like problem in a'],\n",
       " ['00:07:43.080 --> 00:07:43.090', 'GC tasks particular is in term of of you'],\n",
       " ['00:07:45.450 --> 00:07:45.460', 'know that like mainly small number of'],\n",
       " ['00:07:49.410 --> 00:07:49.420', 'local annex to the character sequence of'],\n",
       " ['00:07:52.590 --> 00:07:52.600', 'the source world corresponding to the'],\n",
       " ['00:07:57.150 --> 00:07:57.160', 'target all V word and second is the'],\n",
       " ['00:07:58.950 --> 00:07:58.960', 'character level attention provider'],\n",
       " ['00:08:01.350 --> 00:08:01.360', 'decoded information so that you can'],\n",
       " ['00:08:05.220 --> 00:08:05.230', 'assess the relevant source of character'],\n",
       " ['00:08:10.710 --> 00:08:10.720', 'sequence so our architecture is Cabaye'],\n",
       " ['00:08:14.280 --> 00:08:14.290', 'of the ball like you know particular is'],\n",
       " ['00:08:18.780 --> 00:08:18.790', 'the attention for character level so the'],\n",
       " ['00:08:21.810 --> 00:08:21.820', 'experiment that come with what we did is'],\n",
       " ['00:08:26.310 --> 00:08:26.320', 'we leveraging tree data set you know'],\n",
       " ['00:08:29.280 --> 00:08:29.290', 'from the new co and also from the'],\n",
       " ['00:08:34.470 --> 00:08:34.480', 'Cambridge learning corpus CLC as well as'],\n",
       " ['00:08:38.790 --> 00:08:38.800', 'language ace and we also using the'],\n",
       " ['00:08:43.560 --> 00:08:43.570', 'shares tasks data on the canal 13 for'],\n",
       " ['00:08:47.820 --> 00:08:47.830', 'left test set and also cornea 14 for'],\n",
       " ['00:08:51.810 --> 00:08:51.820', 'testing and we follow basically the mate'],\n",
       " ['00:08:57.630 --> 00:08:57.640', 'which is ever 0.5 score and what we did'],\n",
       " ['00:09:03.120 --> 00:09:03.130', 'is we compared to two cases one is a'],\n",
       " ['00:09:07.430 --> 00:09:07.440', 'pure model compared to the world nmt'],\n",
       " ['00:09:11.640 --> 00:09:11.650', 'plus the unknown replacement as a'],\n",
       " ['00:09:15.360 --> 00:09:15.370', 'baseline and in the this model basically'],\n",
       " ['00:09:19.530 --> 00:09:19.540', 'the anand replacement is a post'],\n",
       " ['00:09:24.540 --> 00:09:24.550', 'processor for the NM t and then on the'],\n",
       " ['00:09:28.410 --> 00:09:28.420', 'hybrid model we just already apply in'],\n",
       " ['00:09:32.730 --> 00:09:32.740', 'the machine translation inspired by long'],\n",
       " ['00:09:39.750 --> 00:09:39.760', 'and ya mean amending am sorry is'],\n",
       " ['00:09:43.170 --> 00:09:43.180', 'basically we we do the hybrid model with'],\n",
       " ['00:09:48.560 --> 00:09:48.570', 'single world level of tension and then'],\n",
       " ['00:09:52.410 --> 00:09:52.420', 'also we compare with the integrated'],\n",
       " ['00:09:57.120 --> 00:09:57.130', \"model with a language model that's done\"],\n",
       " ['00:10:03.840 --> 00:10:03.850', 'by you know say so we cannot lie bubbly'],\n",
       " ['00:10:06.750 --> 00:10:06.760', 'to paper without a number right so so'],\n",
       " ['00:10:09.510 --> 00:10:09.520', 'this is our resource so based on the'],\n",
       " ['00:10:13.140 --> 00:10:13.150', 'comparison our like proposal nested'],\n",
       " ['00:10:16.080 --> 00:10:16.090', 'attention have reach model we got'],\n",
       " ['00:10:19.680 --> 00:10:19.690', 'basically for for the view model we got'],\n",
       " ['00:10:24.930 --> 00:10:24.940', 'forty one point five three and also in'],\n",
       " ['00:10:27.210 --> 00:10:27.220', 'the inter ate it with web-scale'],\n",
       " ['00:10:29.760 --> 00:10:29.770', 'language model we got forty five point'],\n",
       " ['00:10:36.230 --> 00:10:36.240', 'one five so let go into the analysis of'],\n",
       " ['00:10:39.120 --> 00:10:39.130', 'how we can achieve that'],\n",
       " ['00:10:43.920 --> 00:10:43.930', 'you know like score so first we taking'],\n",
       " ['00:10:48.990 --> 00:10:49.000', 'the data set and we divide it into OB'],\n",
       " ['00:10:55.110 --> 00:10:55.120', 'and none OV e case and what we find out'],\n",
       " ['00:11:01.170 --> 00:11:01.180', 'is basically our model is half a fairly'],\n",
       " ['00:11:04.280 --> 00:11:04.290', 'big improvement in terms of all v'],\n",
       " ['00:11:07.819 --> 00:11:07.829', 'and if you look at the sample right I'],\n",
       " ['00:11:11.929 --> 00:11:11.939', 'mean like the model able to correct it'],\n",
       " ['00:11:16.519 --> 00:11:16.529', 'into the after the rodent'],\n",
       " ['00:11:20.529 --> 00:11:20.539', 'set and when you take that ovie case'],\n",
       " ['00:11:26.569 --> 00:11:26.579', 'which is we show it by hybrid model we'],\n",
       " ['00:11:30.529 --> 00:11:30.539', 'also divide into a smallest change in'],\n",
       " ['00:11:33.710 --> 00:11:33.720', \"larger change so here's a definition of\"],\n",
       " ['00:11:36.079 --> 00:11:36.089', 'what what we defined from the Attic'],\n",
       " ['00:11:39.349 --> 00:11:39.359', 'thumb in the small change compared to'],\n",
       " ['00:11:43.369 --> 00:11:43.379', 'latch Ange so so in the sample you see'],\n",
       " ['00:11:46.729 --> 00:11:46.739', 'in front of you they have two to addicts'],\n",
       " ['00:11:50.210 --> 00:11:50.220', 'and these basically based on the saw and'],\n",
       " ['00:11:54.199 --> 00:11:54.209', 'Haggard phrase of addict with a large'],\n",
       " ['00:11:59.599 --> 00:11:59.609', 'Africa similarity or not then we just'],\n",
       " ['00:12:02.840 --> 00:12:02.850', 'generate basically a fly definition a'],\n",
       " ['00:12:06.019 --> 00:12:06.029', 'true base to two separate between a'],\n",
       " ['00:12:09.889 --> 00:12:09.899', 'small change in light change with like'],\n",
       " ['00:12:14.419 --> 00:12:14.429', 'basically we got like 39% for the small'],\n",
       " ['00:12:16.999 --> 00:12:17.009', 'change and the rest is just a light'],\n",
       " ['00:12:21.319 --> 00:12:21.329', 'change so from the analysis we saw that'],\n",
       " ['00:12:26.929 --> 00:12:26.939', 'light proposal we have with a nested'],\n",
       " ['00:12:31.039 --> 00:12:31.049', 'attention hybrid model we got in the'],\n",
       " ['00:12:34.970 --> 00:12:34.980', 'small set is a really large improvement'],\n",
       " ['00:12:40.729 --> 00:12:40.739', 'poor from precision and recon and small'],\n",
       " ['00:12:43.429 --> 00:12:43.439', 'increase in the precision for the latch'],\n",
       " ['00:12:47.149 --> 00:12:47.159', 'change and a small negative for the'],\n",
       " ['00:12:54.499 --> 00:12:54.509', 'record so this leads to the conclusion'],\n",
       " ['00:13:01.519 --> 00:13:01.529', 'that our model is basically able to show'],\n",
       " ['00:13:06.969 --> 00:13:06.979', 'the unique challenges of the GC task and'],\n",
       " ['00:13:10.549 --> 00:13:10.559', 'by basically bringing the both the'],\n",
       " ['00:13:14.689 --> 00:13:14.699', 'global world level and local character'],\n",
       " ['00:13:18.220 --> 00:13:18.230', 'level into a unified way'],\n",
       " ['00:13:22.370 --> 00:13:22.380', 'and as result it generated substantial'],\n",
       " ['00:13:25.190 --> 00:13:25.200', 'improvement that we got so far for'],\n",
       " ['00:13:28.960 --> 00:13:28.970', 'correcting confusion with the rare of'],\n",
       " ['00:13:38.500 --> 00:13:38.510', 'so as conclude our talk so that next for'],\n",
       " ['00:13:41.480 --> 00:13:41.490', 'thank you for attention here and also'],\n",
       " ['00:13:51.140 --> 00:13:51.150',\n",
       "  'next for Q&amp;A okay so we have a couple'],\n",
       " ['00:13:56.300 --> 00:13:56.310', 'minutes for questions hi thank you for'],\n",
       " ['00:13:57.950 --> 00:13:57.960', 'the talk this is Joel tetra from Graham'],\n",
       " ['00:14:00.260 --> 00:14:00.270', 'Lee I have a question on your evaluation'],\n",
       " ['00:14:01.700 --> 00:14:01.710', \"so your training data you're using\"],\n",
       " ['00:14:04.430 --> 00:14:04.440', 'knuckle Langley and CLC did you try'],\n",
       " ['00:14:07.640 --> 00:14:07.650', 'evaluating it with only knuckle or only'],\n",
       " ['00:14:13.460 --> 00:14:13.470', 'knuckle plus lang-8 can you repeat the'],\n",
       " ['00:14:15.680 --> 00:14:15.690', 'last thing did you try a training solely'],\n",
       " ['00:14:18.440 --> 00:14:18.450', 'on knuckle or solely on knuckle plus'],\n",
       " ['00:14:21.920 --> 00:14:21.930', \"lang a just leave out the CLC part it's\"],\n",
       " ['00:14:24.710 --> 00:14:24.720', 'just that lab that one yeah'],\n",
       " ['00:14:26.810 --> 00:14:26.820', 'did you try training on simply the'],\n",
       " ['00:14:30.020 --> 00:14:30.030', 'knuckle corpus and then evaluating so we'],\n",
       " ['00:14:33.610 --> 00:14:33.620', 'knew is with the evaluation not on the'],\n",
       " ['00:14:44.720 --> 00:14:44.730', \"yeah yeah I guess my point is that I'm\"],\n",
       " ['00:14:46.760 --> 00:14:46.770', 'not sure whether your high performance'],\n",
       " ['00:14:48.350 --> 00:14:48.360', 'compared to the other neural models is'],\n",
       " ['00:14:50.420 --> 00:14:50.430', 'due to your model or due to the vast'],\n",
       " ['00:14:51.530 --> 00:14:51.540', \"amount of training data that you're\"],\n",
       " ['00:14:53.900 --> 00:14:53.910', 'using in comparison to the prior work so'],\n",
       " ['00:14:56.810 --> 00:14:56.820', 'she at all 2016 I believed used one I'],\n",
       " ['00:14:59.090 --> 00:14:59.100', 'think three and maybe some artificial'],\n",
       " ['00:15:01.760 --> 00:15:01.770', 'errors but the CLC is like really really'],\n",
       " ['00:15:03.950 --> 00:15:03.960', 'massive and so I wonder if you take that'],\n",
       " ['00:15:05.330 --> 00:15:05.340', 'out do you still have an improvement'],\n",
       " ['00:15:08.960 --> 00:15:08.970', 'over the prior state-of-the-art so the'],\n",
       " ['00:15:13.700 --> 00:15:13.710', 'way we do is basically we we compare we'],\n",
       " ['00:15:15.590 --> 00:15:15.600', 'have a baseline like I mentioned and'],\n",
       " ['00:15:18.520 --> 00:15:18.530', 'then we using the same set that we'],\n",
       " ['00:15:24.880 --> 00:15:24.890', 'evaluate based on like you know like the'],\n",
       " ['00:15:28.850 --> 00:15:28.860', 'canal like share tasks than there okay'],\n",
       " ['00:15:38.640 --> 00:15:38.650', 'yeah sure we try to use our polite I'],\n",
       " ['00:15:42.120 --> 00:15:42.130', \"mean I'm to catch up with the best\"],\n",
       " ['00:15:44.850 --> 00:15:44.860', 'number at a time so we try different'],\n",
       " ['00:15:46.769 --> 00:15:46.779', 'experiment of using different character'],\n",
       " ['00:15:50.070 --> 00:15:50.080', 'training data so we find out that the'],\n",
       " ['00:15:52.560 --> 00:15:52.570', 'commission of this free data is the best'],\n",
       " ['00:15:55.410 --> 00:15:55.420', 'and then for the hybrid experiment and'],\n",
       " ['00:15:57.690 --> 00:15:57.700', 'also the less the experiment we all use'],\n",
       " ['00:16:09.480 --> 00:16:09.490', 'the same training data so we have time'],\n",
       " ['00:16:11.070 --> 00:16:11.080', 'for one more question I know I saw'],\n",
       " ['00:16:22.880 --> 00:16:22.890', 'I think the model is working on'],\n",
       " ['00:16:26.040 --> 00:16:26.050', 'sentence-by-sentence hot about you need'],\n",
       " ['00:16:29.280 --> 00:16:29.290', 'to march to sentence or what about one'],\n",
       " ['00:16:30.990 --> 00:16:31.000', 'sentence is going to split into two'],\n",
       " ['00:16:34.680 --> 00:16:34.690', 'sentence did you have like did you do'],\n",
       " ['00:16:40.070 --> 00:16:40.080', 'anything with that kind of scenario so'],\n",
       " ['00:16:44.430 --> 00:16:44.440', 'what we did is basically for for the'],\n",
       " ['00:16:46.800 --> 00:16:46.810', 'alteration we do sent them to st. Anne'],\n",
       " ['00:16:49.410 --> 00:16:49.420', 'has a pair for the you know like'],\n",
       " ['00:16:52.380 --> 00:16:52.390', 'validation yeah my question is sometimes'],\n",
       " ['00:16:54.450 --> 00:16:54.460', 'you need to split one sentence into two'],\n",
       " ['00:16:57.630 --> 00:16:57.640', 'sentences sometimes even it to Mars to'],\n",
       " ['00:17:00.030 --> 00:17:00.040', 'sentence into all sentences so that'],\n",
       " ['00:17:03.320 --> 00:17:03.330', 'source sentences to target is just one'],\n",
       " ['00:17:06.570 --> 00:17:06.580', \"sometimes it's vice versa did you have\"],\n",
       " ['00:17:08.280 --> 00:17:08.290', 'this kind of scenario did you deal with'],\n",
       " ['00:17:10.230 --> 00:17:10.240', 'this kind of scenario your your model is'],\n",
       " ['00:17:12.020 --> 00:17:12.030', 'able to deal with this kind of scenario'],\n",
       " ['00:17:15.090 --> 00:17:15.100', \"that's a first-person so that's a that's\"],\n",
       " ['00:17:17.670 --> 00:17:17.680', 'a good good I you know like question I'],\n",
       " ['00:17:21.449 --> 00:17:21.459', 'think we right now we try basically'],\n",
       " ['00:17:24.240 --> 00:17:24.250', 'based on the pairs of sentence to'],\n",
       " ['00:17:35.270 --> 00:17:35.280', 'okay what about paragraph level'],\n",
       " ['00:17:38.310 --> 00:17:38.320', 'information say one sentence is talking'],\n",
       " ['00:17:41.640 --> 00:17:41.650', 'about a subject in send and so on but'],\n",
       " ['00:17:43.440 --> 00:17:43.450', 'you need that information in sentence'],\n",
       " ['00:17:47.220 --> 00:17:47.230', 'pipe is the model is considering sent'],\n",
       " ['00:17:50.700 --> 00:17:50.710', \"and so on for oil it's working on\"],\n",
       " ['00:17:54.570 --> 00:17:54.580', \"sentence five no we don't do that right\"],\n",
       " ['00:18:01.540 --> 00:18:04.559', '[Applause]']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = open(wd+\"/AI_courses/A Nested Attention Neural Hybrid Model for Grammatical Error Correction _ ACL 2017-SI9cTcyNVew.en.vtt\", \"r\", encoding='utf-8').read()\n",
    "vtt0 = test.split('\\n\\n')\n",
    "\n",
    "def cleanAutogenSubtitles(subtitles):\n",
    "    line = [item for item in subtitles[2].split('\\n') if item is not ' ']\n",
    "    try:\n",
    "        if(len(line[0].split(' --> ')[1]) > 12):\n",
    "            cleaned = []\n",
    "            for x in subtitles: \n",
    "                line = [item for item in x.split('\\n') if item is not ' ']\n",
    "                line[0] = line[0][:29]\n",
    "                if(len(line) == 2 and len(cleanhtml(line[1])) == len(line[1])):\n",
    "                    cleaned.append(line)\n",
    "            subtitles = cleaned\n",
    "    except:\n",
    "        return subtitles\n",
    "    \n",
    "    return subtitles\n",
    "            \n",
    "cleanAutogenSubtitles(vtt0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 of 117\r"
     ]
    }
   ],
   "source": [
    "next(os.walk('.'))[1]\n",
    "\n",
    "subdir_ls = [(x[0],x[2]) for x in os.walk(wd) if len(x[2])>0] #only get directories with files\n",
    "\n",
    "#part 0: Create dict of files\n",
    "file_name_ls = []\n",
    "file_content_ls = []\n",
    "for i, subdir in enumerate(subdir_ls):\n",
    "    print(str(i+1)+' of '+str(len(subdir_ls)), end='\\r')\n",
    "    subdir_path, subdir_files = subdir[0], subdir[1]\n",
    "    file_name_ls += subdir_files\n",
    "    for f in subdir_files:\n",
    "        vtt0 = open(subdir_path+\"/\"+f, \"r\", encoding='utf-8').read()\n",
    "        vtt1 = cleanAutogenSubtitles(vtt0.split('\\n\\n'))\n",
    "        if vtt1 != ['']:\n",
    "            vtt2 = []\n",
    "            for x in vtt1:\n",
    "                if(len(x)>0):\n",
    "                    try:\n",
    "                        tmp = x.split('\\n')\n",
    "                        timepoint = tmp[0].split(' --> ')\n",
    "                        timepoint = timepoint[0] + '\\t' + timepoint[1]   #convert to string\n",
    "                        label = f + '\\t' + timepoint\n",
    "                        vtt2.append((label, ' '.join(tmp[1:]).lower()))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            vtt1 = vtt2\n",
    "        file_content_ls.append(vtt1)\n",
    "    \n",
    "file_content_dict = dict(zip(file_name_ls,file_content_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2614 of 2614\r"
     ]
    }
   ],
   "source": [
    "writer = ix.writer()\n",
    "length = len(file_content_dict)\n",
    "for i, f in enumerate(file_content_dict):\n",
    "    print(f'{i+1} of {length}', end='\\r')\n",
    "    tmp = ''\n",
    "    for caption in file_content_dict[f]:\n",
    "        tmp += caption[1] + ' '\n",
    "        \n",
    "    writer.add_document(title=f, path=\"/\"+f, content=tmp)\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from whoosh.qparser import QueryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toseconds(timepoint):\n",
    "    tmp = timepoint.split('.')[0].split(':')\n",
    "    hours = int(tmp[0])\n",
    "    minutes = int(tmp[1])\n",
    "    seconds = int(tmp[2])\n",
    "    return str(hours*360 + minutes*60 + seconds)\n",
    "\n",
    "def searchCorpus(q):\n",
    "    with ix.searcher() as searcher:\n",
    "        query = QueryParser('content', ix.schema).parse(q)\n",
    "        results = searcher.search(query, limit=10)\n",
    "        results.fragmenter.charlimit = None\n",
    "        # Show more context before and after\n",
    "        results.fragmenter.surround = 400\n",
    "        for hit in results:\n",
    "            print(hit['title'][:-19] + '\\n')\n",
    "\n",
    "            filecontents = ''\n",
    "            for line in file_content_dict[hit['title']]:\n",
    "                filecontents += '\\n' + line[0] + '\\t' + line[1]\n",
    "            \n",
    "            snippets = hit.highlights('content', text=filecontents, top=5).split('...')\n",
    "            snippets = [cleanhtml(x) for x in snippets]\n",
    "            for snippet in snippets:\n",
    "                surrounding_lines = snippet.split('\\n')[1:-1]\n",
    "                for i, line in enumerate(surrounding_lines):\n",
    "                    tmp = line.split('\\t')\n",
    "                    print('\\t' + tmp[3])\n",
    "                    \n",
    "                    if(i == 0):\n",
    "                        video_id = tmp[0][-18:-7]\n",
    "                        time_start = toseconds(tmp[1])\n",
    "                \n",
    "                print('\\thttps://youtu.be/' + video_id + '?t=' + time_start + '\\n')\n",
    "                \n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lec 10 _ MIT 6.00 Introduction to Computer Science and Programming, Fall 2008\n",
      "\n",
      "\tbut it does better than even this kind of binary search,\n",
      "\tand that's a method called hashing.\n",
      "\tyou've actually seen hashing, you just don't know it.\n",
      "\thttps://youtu.be/kDhR4Zm53zc?t=1193\n",
      "\n",
      "\thashing is used every time you enter a password into a system.\n",
      "\tso what in the world is hashing?\n",
      "\thttps://youtu.be/kDhR4Zm53zc?t=1220\n",
      "\n",
      "\twhat have we done over the last three or four lectures?\n",
      "\twe've started introducing you to classes of algorithms.\n",
      "\tthings that i'd like you to be able to see\n",
      "\thttps://youtu.be/kDhR4Zm53zc?t=1765\n",
      "\n",
      "\tthese are tools that you want in your tool box.\n",
      "\tthese are the kinds of algorithms that you\n",
      "\tshould be able to recognize.\n",
      "\thttps://youtu.be/kDhR4Zm53zc?t=1837\n",
      "\n",
      "\tnext week, and what we're going to start doing then is taking\n",
      "\tthese classes of algorithms and start looking at much more\n",
      "\tcomplex algorithms.\n",
      "\thttps://youtu.be/kDhR4Zm53zc?t=1867\n",
      "\n",
      "R5. Dynamic Programming\n",
      "\n",
      "\tok.\n",
      "\tany ideas how to solve that?\n",
      "\tso let's just use a naive or very straightforward\n",
      "\talgorithms.\n",
      "\tgo ahead.\n",
      "\taudience: you pick one, and then you do mc of n minus that.\n",
      "\thttps://youtu.be/krZI60lKPek?t=408\n",
      "\n",
      "\tbut nancy, one of the lecturers suggested,\n",
      "\tthat it seems that many people have some trouble understanding\n",
      "\tyesterday's lecture on universal hashing and perfect hashing.\n",
      "\tso we can also consider going through that.\n",
      "\thttps://youtu.be/krZI60lKPek?t=1960\n",
      "\n",
      "\tin the hash function.\n",
      "\ti'm not sure why.\n",
      "\tnow once we have that, once we have universal hash,\n",
      "\tpeople also want perfect hashing,\n",
      "\twhich means i want absolutely 0 collision.\n",
      "\tso how do i do that?\n",
      "\thttps://youtu.be/krZI60lKPek?t=2689\n",
      "\n",
      "\ti'll choose each of the bins to be\n",
      "\ta second level perfect hashing.\n",
      "\tso we can use the method one to choose this small one.\n",
      "\thttps://youtu.be/krZI60lKPek?t=2984\n",
      "\n",
      "\tquestion?\n",
      "\taudience: it was mentioned in the lecture\n",
      "\thttps://youtu.be/krZI60lKPek?t=3028\n",
      "\n",
      "8. Randomization - Universal & Perfect Hashing\n",
      "\n",
      "\tnecessary for randomized algorithms,\n",
      "\thttps://youtu.be/z0lJ2k0sl1g?t=643\n",
      "\n",
      "\thttps://youtu.be/z0lJ2k0sl1g?t=643\n",
      "\n",
      "\tto find a nearby prime number.\n",
      "\thttps://youtu.be/z0lJ2k0sl1g?t=1941\n",
      "\n",
      "\tall right.\n",
      "\tthat ends universal hashing.\n",
      "\thttps://youtu.be/z0lJ2k0sl1g?t=3181\n",
      "\n",
      "\tbecause it was invented by fredman, komlos, and szemeredi\n",
      "\thttps://youtu.be/z0lJ2k0sl1g?t=3411\n",
      "\n",
      "Lec 8 _ MIT 6.046J _ 18.410J Introduction to Algorithms (SMA 5503), Fall 2005\n",
      "\n",
      "\thttps://youtu.be/z0lJ2k0sl1g?t=3411\n",
      "\n",
      "\thashing, ok, is one way of making this scheme work.\n",
      "\thttps://youtu.be/s7QSM_hlS1U?t=311\n",
      "\n",
      "\thashing.\n",
      "\thttps://youtu.be/s7QSM_hlS1U?t=2981\n",
      "\n",
      "\thashing is good in the expected sense.\n",
      "\thttps://youtu.be/s7QSM_hlS1U?t=2999\n",
      "\n",
      "\there. so level one hashing,\n",
      "\thttps://youtu.be/s7QSM_hlS1U?t=3422\n",
      "\n",
      "14. Sorting in Linear Time\n",
      "\n",
      "\ttrying to beat that.\n",
      "\ttrying to get rid of the log.\n",
      "\tthere's a counting sort from undergraduate algorithms.\n",
      "\tthat takes n plus u time because you need\n",
      "\tto build a table of size u.\n",
      "\thttps://youtu.be/pOKy3RZbSws?t=233\n",
      "\n",
      "\thow to get linear time.\n",
      "\tand this is fun.\n",
      "\tit uses bit tricks, it uses a lot of cool ideas,\n",
      "\tuses hashing, lots of things.\n",
      "\tit also uses another sorting algorithm called packed sort.\n",
      "\thttps://youtu.be/pOKy3RZbSws?t=741\n",
      "\n",
      "\tto the epsilon n of them.\n",
      "\tso we should be able to reduce to have a much smaller universe\n",
      "\tusing hashing.\n",
      "\tmaybe perfect hashing even.\n",
      "\ti mean, we don't have to be too intelligent here.\n",
      "\thttps://youtu.be/pOKy3RZbSws?t=1034\n",
      "\n",
      "\tby a birthday paradox, constant probability none of them\n",
      "\twill hit each other.\n",
      "\tthat was the second level of perfect hashing.\n",
      "\twe don't need two levels here, just say ok, user order\n",
      "\tlog n bits.\n",
      "\thttps://youtu.be/pOKy3RZbSws?t=1088\n",
      "\n",
      "\tcool.\n",
      "\tso at this point we have mucked up all the digits.\n",
      "\tthe annoying thing about this hashing\n",
      "\tis we do not preserve order.\n",
      "\tit's a little weird for a sorting algorithm not\n",
      "\thttps://youtu.be/pOKy3RZbSws?t=1211\n",
      "\n",
      "16. Strings\n",
      "\n",
      "\tit's only t space, not t sigma.\n",
      "\tother questions?\n",
      "\tor answers?\n",
      "\tthere's a problem with hashing--\n",
      "\tdoesn't actually solve the problem we want to solve.\n",
      "\tit doesn't solve predecessor.\n",
      "\thttps://youtu.be/NinWEPPrkDQ?t=1024\n",
      "\n",
      "\tthis is the problem we had with--\n",
      "\twhat's it called-- signature sort, which hashed,\n",
      "\thttps://youtu.be/NinWEPPrkDQ?t=1042\n",
      "\n",
      "\tis this string in your set?\n",
      "\tbut it won't solve the predecessor problem.\n",
      "\tso this is an interesting solution-- hashing--\n",
      "\tbut not quite what we want.\n",
      "\tand van emde boas doesn't quite do what we want either.\n",
      "\thttps://youtu.be/NinWEPPrkDQ?t=1106\n",
      "\n",
      "\tcost?\n",
      "\tand it depends on our representation,\n",
      "\tit depends how quickly we can traverse a node.\n",
      "\tif we use hashing--\n",
      "\tmethod 3-- use hashing, then we get order p time.\n",
      "\tbut the trouble with hashing is it permutes\n",
      "\thttps://youtu.be/NinWEPPrkDQ?t=2976\n",
      "\n",
      "\there, p plus log sigma.\n",
      "\tsmall penalty to pay but the nice thing is then your answers\n",
      "\tare represented in order.\n",
      "\tno permutation, no hashing, no randomization.\n",
      "\tthis is the reason suffix trees were invented--\n",
      "\tthey let you do searches fast.\n",
      "\thttps://youtu.be/NinWEPPrkDQ?t=3043\n",
      "\n",
      "10. Dictionaries\n",
      "\n",
      "\ta lot of the results we'll see require log n independence.\n",
      "\tbut the cool thing is, roughly speaking simple tabulation\n",
      "\thashing is almost as good as log n wise independence\n",
      "\tin all the hashing schemes that we care about.\n",
      "\thttps://youtu.be/Mf9Nn9PbGsE?t=1004\n",
      "\n",
      "\tthis is a sense in which it's ok.\n",
      "\tdon't worry about it.\n",
      "\tall right, but if you did worry about it, next thing you do\n",
      "\tis perfect hashing.\n",
      "\tso, perfect hashing is really just an embellishment.\n",
      "\tthis is also called fks hashing.\n",
      "\thttps://youtu.be/Mf9Nn9PbGsE?t=1960\n",
      "\n",
      "\twhat about real hash functions?\n",
      "\twe can't use totally random.\n",
      "\twhat about universal k-wise independent simple tabulation\n",
      "\thashing, just for chaining?\n",
      "\tok, and similar things hold for perfect hashing, i think.\n",
      "\ti'm not sure if they're all known.\n",
      "\thttps://youtu.be/Mf9Nn9PbGsE?t=2393\n",
      "\n",
      "\ti'm going to go to something kind of related double hashing,\n",
      "\twhich is cuckoo hashing.\n",
      "\thttps://youtu.be/Mf9Nn9PbGsE?t=970\n",
      "\n",
      "\tit's kind of a more extreme form of perfect hashing.\n",
      "\thttps://youtu.be/Mf9Nn9PbGsE?t=989\n",
      "\n",
      "Lec 7 _ MIT 6.046J _ 18.410J Introduction to Algorithms (SMA 5503), Fall 2005\n",
      "\n",
      "\tfunction h which maps the keys randomly.\n",
      "\thttps://youtu.be/JZHBa-rLrBA?t=596\n",
      "\n",
      "\tright? they teach it in?\n",
      "\thttps://youtu.be/JZHBa-rLrBA?t=1009\n",
      "\n",
      "\tso what happens in the worst case?\n",
      "\thttps://youtu.be/JZHBa-rLrBA?t=1038\n",
      "\n",
      "\tin the worst case, hashing is lousy.\n",
      "\thttps://youtu.be/JZHBa-rLrBA?t=614\n",
      "\n",
      "\teach key is equally likely to have any one of the m factorial\n",
      "\thttps://youtu.be/JZHBa-rLrBA?t=880\n",
      "\n",
      "11. Integer Models\n",
      "\n",
      "\tand this is not really our first,\n",
      "\tbut it is an example of an integer data structure.\n",
      "\tand for whatever reason, i don't brand\n",
      "\thashing as an integer data structure,\n",
      "\tjust because it's its own beast.\n",
      "\tbut in particular, today, i need to be a little more\n",
      "\thttps://youtu.be/u-HHY1ylhHY?t=48\n",
      "\n",
      "\tinsert, delete, predecessor, successor,\n",
      "\tthere are actually lower bounds that say you cannot do better\n",
      "\tthan such and such.\n",
      "\twith hashing, there aren't really any lower bounds,\n",
      "\tbecause you can do everything in constant time\n",
      "\twith high probability.\n",
      "\thttps://youtu.be/u-HHY1ylhHY?t=72\n",
      "\n",
      "\tand order n space.\n",
      "\twith a slight tweak, basically, you combine van emde boas\n",
      "\tplus hashing, and you get that.\n",
      "\ti don't actually know what the reference is for this result.\n",
      "\thttps://youtu.be/u-HHY1ylhHY?t=1014\n",
      "\n",
      "\tso don't use an array.\n",
      "\tuse a perfect hash table.\n",
      "\tso v dot cluster, instead of being an array,\n",
      "\tis now, let's say, a dynamic perfect hashing.\n",
      "\tand i'm going to use the version which i did not present.\n",
      "\tthe version i presented, which used universal hashing,\n",
      "\thttps://youtu.be/u-HHY1ylhHY?t=1083\n",
      "\n",
      "\tand you get log w per operation order and space.\n",
      "\tof course, this is a high probability\n",
      "\tbecause we're using hashing.\n",
      "\tbecause we have a factor w bad here,\n",
      "\twe have factor w bad here.\n",
      "\thttps://youtu.be/u-HHY1ylhHY?t=1525\n",
      "\n",
      "21. 3SUM and APSP Hardness\n",
      "\n",
      "\tby testing all triples, whether they sum to 0.\n",
      "\tbut you can also solve it in quadratic time.\n",
      "\tso it's not an algorithms class.\n",
      "\ti won't ask you to come up the algorithm.\n",
      "\tbut they're quite easy.\n",
      "\thttps://youtu.be/X05j49pc6DE?t=89\n",
      "\n",
      "\tmore generally, there are a bunch\n",
      "\tof sub-quadratic, but only slightly sub-quadratic,\n",
      "\talgorithms.\n",
      "\tthe first one achieves a roughly log squared savings.\n",
      "\thttps://youtu.be/X05j49pc6DE?t=307\n",
      "\n",
      "\tand very recently, this year, there's\n",
      "\tbeen another nice improvement.\n",
      "\tit's actually three algorithms, depending on your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbut all based on a similar idea.\n",
      "\tdid i get these backwards?\n",
      "\thttps://youtu.be/X05j49pc6DE?t=379\n",
      "\n",
      "\tand actually several problems in computers since the '80s,\n",
      "\ti think, where we know better decision trees\n",
      "\tthan we know algorithms.\n",
      "\tso my sense would be that decision tree model is strictly\n",
      "\tmore powerful.\n",
      "\thttps://youtu.be/X05j49pc6DE?t=587\n",
      "\n",
      "\tyou just have to weaken this statement.\n",
      "\tbut here i'll say deterministic.\n",
      "\taudience: is the hashing argument for why [inaudible]?\n",
      "\tprofessor: yeah, i'm pretty sure you can derandomize\n",
      "\thttps://youtu.be/X05j49pc6DE?t=1394\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Top 10 Results for And([Term('content', 'hashing'), Term('content', 'algorithms')]) runtime=0.0013288339996506693>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCorpus('hashing algorithms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
